# Kaggle-Titanic
Attempt on the Titanic dataset from Kaggle


Some of the models that I will be attempting to use are:

1) Logistic Regression
2) Decision Trees
3) Random Forest
4) Gradient Boosting
5) XGBoost
6) To be further extended with more knowledge.

# Attempts:

## 1) Attempt 1.1 on 4th October 2020:

Columns used: Age, Sex, Scaled Fare, Dummy variables: EmbarkedC, EmbarkedS, Pclass_Upper, Pclass_Middle

Accuracy on test set: 0.76794

## 2) Attempt 1.2 on 5th October 2020:

Columns used: Sex, Age,	SibSp, Fare, EmbarkedS, Pclass_Middle, Pclass_Lower

Accuracy on test set: 0.76794

## 3) Attempt 2.1 on 5th October 2020:

Model: Decision Trees, 'Gini' Criterion

Columns used: Sex, Age,	SibSp, Fare, EmbarkedS, Pclass_Middle, Pclass_Lower

Accuracy on test set: 0.70813 

## 4) Attempt 2.2 on 5th October 2020:

Model: Decision Trees, 'Entropy' Criterion

Columns used: Sex, Age,	SibSp, Fare, EmbarkedS, Pclass_Middle, Pclass_Lower

Accuracy on test set: 0.77511 


# Overall reflections and evaluations:

